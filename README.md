# Temporal-Frequency Co-training for Time Series Semi-supervised Learning
This is the training code for our paper [*"Temporal-Frequency Co-training for Time Series Semi-supervised Learning"*](https://doi.org/10.1609/aaai.v37i7.26072) (AAAI-23).

## Overview

Semi-supervised learning (SSL) has been actively studied due to its ability to alleviate the reliance of deep learning models on labeled data. Although existing SSL methods based on pseudo-labeling strategies have made great progress, they rarely consider time-series data's intrinsic properties (e.g., temporal dependence). Learning representations by mining the inherent properties of time series has recently gained much attention. Nonetheless, how to utilize feature representations to design SSL paradigms for time series has not been explored. To this end, we propose a Time Series SSL framework via Temporal-Frequency Co-training (TS-TFC), leveraging the complementary information from two distinct views for unlabeled data learning. In particular, TS-TFC employs time-domain and frequency-domain views to train two deep neural networks simultaneously, and each view's pseudo-labels generated by label propagation in the representation space are adopted to guide the training of the other view's classifier. To enhance the discriminative of representations between categories, we propose a temporal-frequency supervised contrastive learning module, which integrates the learning difficulty of categories to improve the quality of pseudo-labels. Through co-training the pseudo-labels obtained from temporal-frequency representations, the complementary information in the two distinct views is exploited to enable the model to better learn the distribution of categories. Extensive experiments on 106 UCR datasets show that TS-TFC outperforms state-of-the-art methods, demonstrating the effectiveness and robustness of our proposed model.


## Data
The datasets used in this paper are as follows:
* [128 UCR datasets](https://www.cs.ucr.edu/~eamonn/time_series_data_2018/UCRArchive_2018.zip)

To obtain stable evaluation results, we limit the average number of samples included in each category to at least 30.
Therefore, we utilize 106 datasets from the original 128 UCR datasets for experimental analysis.

## Usage
To train a TS-TFC model on a dataset, run

```bash
python ts_tfc_ssl/main_ts_tfc.py --dataset [name of the dataset you want to train]  --labeled_ratio [labeling ratio of the dataset] ...
```
For detailed options and examples, please refer to parser setup in ```ts_tfc_ssl/main_ts_tfc.py```

## Citation
If you use this code for your research, please cite our paper:
```
@inproceedings{liu2023tstfc,
  title={Temporal-Frequency Co-training for Time Series Semi-supervised Learning},
  author={Liu, Zhen and Ma, Qianli and Ma, Peitian and Wang, Linghao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={7},
  pages={8923--8931},
  year={2023}
}
```
